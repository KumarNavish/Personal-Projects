{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text_Classification_using_Word2Vec_Embeddings.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "YEq7kYLMMW_p",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Just run the cells marked with star[****] to produce features and labels:"
      ]
    },
    {
      "metadata": {
        "id": "0mIfLglOMW_r",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Importing 20_Newsgroups Public Dataset:"
      ]
    },
    {
      "metadata": {
        "id": "5rgDJzDWMW_t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#***************************************************************************************\n",
        "import numpy as np\n",
        "import pickle\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "train = fetch_20newsgroups(subset='train')\n",
        "test = fetch_20newsgroups(subset='test')\n",
        "#combining training and testing dataset\n",
        "train_x,y_train = train.data,train.target \n",
        "test_x,y_test = test.data,test.target\n",
        "data = train_x+test_x\n",
        "targets = np.array(y_train.tolist()+y_test.tolist())\n",
        "#***************************************************************************************"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9AjeUHPNMW_y",
        "colab_type": "code",
        "colab": {},
        "outputId": "d61bfc74-ca8f-4a53-843d-cd0e0d1d3031"
      },
      "cell_type": "code",
      "source": [
        "#A glimpse into individual Document/email:\n",
        "data[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"From: lerxst@wam.umd.edu (where's my thing)\\nSubject: WHAT car is this!?\\nNntp-Posting-Host: rac3.wam.umd.edu\\nOrganization: University of Maryland, College Park\\nLines: 15\\n\\n I was wondering if anyone out there could enlighten me on this car I saw\\nthe other day. It was a 2-door sports car, looked to be from the late 60s/\\nearly 70s. It was called a Bricklin. The doors were really small. In addition,\\nthe front bumper was separate from the rest of the body. This is \\nall I know. If anyone can tellme a model name, engine specs, years\\nof production, where this car is made, history, or whatever info you\\nhave on this funky looking car, please e-mail.\\n\\nThanks,\\n- IL\\n   ---- brought to you by your neighborhood Lerxst ----\\n\\n\\n\\n\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 145
        }
      ]
    },
    {
      "metadata": {
        "id": "YaSmiLmPMW_5",
        "colab_type": "code",
        "colab": {},
        "outputId": "fc7cd200-5878-4a0d-8d46-373023c913bd"
      },
      "cell_type": "code",
      "source": [
        "#News Genres:\n",
        "train.target_names"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['alt.atheism',\n",
              " 'comp.graphics',\n",
              " 'comp.os.ms-windows.misc',\n",
              " 'comp.sys.ibm.pc.hardware',\n",
              " 'comp.sys.mac.hardware',\n",
              " 'comp.windows.x',\n",
              " 'misc.forsale',\n",
              " 'rec.autos',\n",
              " 'rec.motorcycles',\n",
              " 'rec.sport.baseball',\n",
              " 'rec.sport.hockey',\n",
              " 'sci.crypt',\n",
              " 'sci.electronics',\n",
              " 'sci.med',\n",
              " 'sci.space',\n",
              " 'soc.religion.christian',\n",
              " 'talk.politics.guns',\n",
              " 'talk.politics.mideast',\n",
              " 'talk.politics.misc',\n",
              " 'talk.religion.misc']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 147
        }
      ]
    },
    {
      "metadata": {
        "id": "aPKo1Dk-MW__",
        "colab_type": "code",
        "colab": {},
        "outputId": "1472aa83-48ac-4262-be1b-06c5e7bae3be"
      },
      "cell_type": "code",
      "source": [
        "#Shape of training and testing data:\n",
        "y_train.shape,y_test.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((11314,), (7532,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "metadata": {
        "id": "9C8SG61PMXAF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Tokenizing and cleaning the text using NLTK"
      ]
    },
    {
      "metadata": {
        "id": "wcEwc3odMXAH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import re \n",
        "import pickle\n",
        "from nltk.corpus import wordnet \n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "stopwords = list(stopwords.words('english'))+['From','Subject','Organization','Lines','\\n']\n",
        "tokenizer = TreebankWordTokenizer()\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Qtp7W3l_MXAK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## This cell is refining the dataset by removing punctuations, and doing afterwards tokenization and lemmatization.\n",
        "* I have already run this cell and stored refined training and testing datsets in pickle format [__See cell below this cell__]."
      ]
    },
    {
      "metadata": {
        "id": "Y-2DmWyNMXAM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def lemmatization(text_tuple_list):\n",
        "    new_text = []\n",
        "    for word,tag in text_tuple_list:\n",
        "        if word not in stopwords: #stopwords removal\n",
        "            if tag.startswith('J'):\n",
        "                new_text.append(lemmatizer.lemmatize(word,wordnet.ADJ)) #giving the proper Part-of-speech\n",
        "            elif tag.startswith('V'):\n",
        "                new_text.append(lemmatizer.lemmatize(word,wordnet.VERB)) \n",
        "            elif tag.startswith('N'):\n",
        "                new_text.append(lemmatizer.lemmatize(word,wordnet.NOUN))\n",
        "            elif tag.startswith('R'):\n",
        "                new_text.append(lemmatizer.lemmatize(word,wordnet.ADV))\n",
        "            else:\n",
        "                new_text.append(lemmatizer.lemmatize(word,wordnet.NOUN))\n",
        "    return new_text\n",
        "\n",
        "def get_words_only(doc): #doc: the entire dataset.\n",
        "    ref_text = [] \n",
        "    for text in doc: #text:individual email.\n",
        "        ref_text.append(lemmatization(nltk.pos_tag(tokenizer.tokenize(' '.join(re.findall(r'\\w+',text)))))) \n",
        "        # Above code-line: Puntuation removal,Tokenization & Lemmatization in respective order.\n",
        "    return ref_text\n",
        "ref_data = get_words_only(data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l1wV6pVUMXAQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# with open('refined_data.pkl','wb') as fp:\n",
        "#     pickle.dump(ref_data,fp)\n",
        "with open('refined_data.pkl','rb') as fp:\n",
        "    ref_data = pickle.load(fp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "y6MXKDTOMXAV",
        "colab_type": "code",
        "colab": {},
        "outputId": "f1aeddfb-3d5d-432f-b8e9-bcd2621d231d"
      },
      "cell_type": "code",
      "source": [
        "# Unrefined data:\n",
        "data[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"From: lerxst@wam.umd.edu (where's my thing)\\nSubject: WHAT car is this!?\\nNntp-Posting-Host: rac3.wam.umd.edu\\nOrganization: University of Maryland, College Park\\nLines: 15\\n\\n I was wondering if anyone out there could enlighten me on this car I saw\\nthe other day. It was a 2-door sports car, looked to be from the late 60s/\\nearly 70s. It was called a Bricklin. The doors were really small. In addition,\\nthe front bumper was separate from the rest of the body. This is \\nall I know. If anyone can tellme a model name, engine specs, years\\nof production, where this car is made, history, or whatever info you\\nhave on this funky looking car, please e-mail.\\n\\nThanks,\\n- IL\\n   ---- brought to you by your neighborhood Lerxst ----\\n\\n\\n\\n\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 150
        }
      ]
    },
    {
      "metadata": {
        "id": "2vPP4ph6MXAe",
        "colab_type": "code",
        "colab": {},
        "outputId": "92b8ea5b-63d3-4d69-c4bc-3a305734d58c"
      },
      "cell_type": "code",
      "source": [
        "# After Refining:\n",
        "' '.join(ref_data[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'lerxst wam umd edu thing WHAT car Nntp Posting Host rac3 wam umd edu University Maryland College Park 15 I wonder anyone could enlighten car I saw day It 2 door sport car look late 60 early 70 It call Bricklin The door really small In addition front bumper separate rest body This I know If anyone tellme model name engine spec year production car make history whatever info funky look car please e mail Thanks IL bring neighborhood Lerxst'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 151
        }
      ]
    },
    {
      "metadata": {
        "id": "hP2il3ncMXAl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Word2Vec using Gensim:"
      ]
    },
    {
      "metadata": {
        "id": "aO8Wam2kMXAm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "# import word2vec model from gensim\n",
        "# In here we are making 32 length vectors for words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Mb7AtVeKMXAq",
        "colab_type": "code",
        "colab": {},
        "outputId": "01fa6850-16f4-421e-9652-14cdcc6d1c36"
      },
      "cell_type": "code",
      "source": [
        "model = gensim.models.Word2Vec(ref_data, min_count=2, size = 32,workers=4) # Training documents\n",
        "model.save('TrainedModel')\n",
        "# model = gensim.models.Word2Vec.load('TrainedModel')\n",
        "list(model.wv.vocab.keys())[:10] #Listing out some words in the vocabulary"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['lerxst',\n",
              " 'wam',\n",
              " 'umd',\n",
              " 'edu',\n",
              " 'thing',\n",
              " 'WHAT',\n",
              " 'car',\n",
              " 'Nntp',\n",
              " 'Posting',\n",
              " 'Host']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 154
        }
      ]
    },
    {
      "metadata": {
        "id": "jyJx_ogJMXAv",
        "colab_type": "code",
        "colab": {},
        "outputId": "0e82d693-a972-4a89-f9a9-b3f6a5f122df"
      },
      "cell_type": "code",
      "source": [
        "model['university'] # Vector of length 32 for word \"university\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/home/navish/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-1.310268  , -1.3606578 , -0.23493813,  0.8143682 ,  0.17923371,\n",
              "       -0.11570183, -0.5684594 , -0.77626234, -0.29843172,  1.8071996 ,\n",
              "       -1.3042477 ,  0.45951122,  0.18712486, -1.35923   ,  1.6233407 ,\n",
              "        0.04698589,  0.8572935 ,  0.31643814, -2.1183898 , -0.7802545 ,\n",
              "       -0.69086695,  0.36233494, -1.2134007 , -1.4227774 , -0.6366987 ,\n",
              "        0.8359502 , -0.68532044, -0.6712709 , -0.23325507, -0.92290527,\n",
              "       -0.795331  , -0.446155  ], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 155
        }
      ]
    },
    {
      "metadata": {
        "id": "HF2cQNbwMXA1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Making all sentences of same length:\n",
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=10, whiten=True) \n",
        "## Restricting the sentence length to 10 words for avoiding computational Complexity."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d3MymRg_MXA8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Getting Features for our model"
      ]
    },
    {
      "metadata": {
        "id": "ln2wg9NTMXA9",
        "colab_type": "code",
        "colab": {},
        "outputId": "ca0eba8e-b504-4348-b18a-53d75ba5e299"
      },
      "cell_type": "code",
      "source": [
        "# Features:\n",
        "def get_train_WordVectors(doc):\n",
        "    text2vec = []\n",
        "    for text_list in doc:\n",
        "        temp=[]\n",
        "        for word in text_list:\n",
        "            if word in train_model.wv.vocab.keys():\n",
        "                temp.append(train_model[word]) # Using vectors for words\n",
        "        # getting features of dim:10*32\n",
        "        text2vec.append(np.transpose(pca.fit_transform(np.transpose(np.array(temp))))) \n",
        "    return np.array(text2vec)\n",
        "features = get_train_WordVectors(ref_data) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/home/navish/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "QoO6qXtrMXBE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Splitting into train and test:\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=0)\n",
        "for train_index, test_index in sss.split(features, targets):\n",
        "    train_features, test_features = features[train_index], features[test_index]\n",
        "    y_train, y_test = targets[train_index], targets[test_index]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LFhGYzBkMXBI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Shapes for training and testing data:"
      ]
    },
    {
      "metadata": {
        "id": "d6DciCnpMXBK",
        "colab_type": "code",
        "colab": {},
        "outputId": "39c772ac-56a0-43be-d1a8-d45e7db6e4c8"
      },
      "cell_type": "code",
      "source": [
        "train_features.shape,test_features.shape ## Here 32 is the length of word vectors"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((13192, 10, 32), (5654, 10, 32))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 162
        }
      ]
    },
    {
      "metadata": {
        "id": "oJy5G6JmMXBP",
        "colab_type": "code",
        "colab": {},
        "outputId": "3c2de565-051a-4a86-dbe0-404396135cd2"
      },
      "cell_type": "code",
      "source": [
        "train_features[0][:2]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.23186775, -0.6520217 , -0.4004753 , -1.1607968 ,  0.03996227,\n",
              "         0.08741288,  0.7559615 , -0.40436098, -0.6398477 , -1.4138429 ,\n",
              "         1.8241627 ,  0.746278  , -0.8856913 ,  0.25858217,  0.16521806,\n",
              "        -1.9235377 ,  0.11031622, -1.2854174 ,  1.5676588 ,  0.28131175,\n",
              "         1.2927779 ,  0.6763326 ,  0.19645643,  2.588946  , -0.5486543 ,\n",
              "        -0.8266543 , -0.38063905, -1.2012979 ,  0.22351465, -0.63498795,\n",
              "         0.18617636,  1.1252905 ],\n",
              "       [ 0.721735  , -1.5837886 ,  0.17612976, -1.3189217 , -0.68496716,\n",
              "         0.15538706,  1.0982243 ,  1.321961  , -0.08356301,  0.61828643,\n",
              "        -0.51905197, -0.97500336,  0.22576787, -1.5622381 ,  0.46475992,\n",
              "        -1.5012553 , -0.98459566,  0.6308646 , -0.35756305, -0.48749956,\n",
              "         1.1084393 , -0.40805563, -0.0325205 , -1.5285044 , -1.3362103 ,\n",
              "         0.8110064 ,  1.092042  ,  0.5011803 ,  1.5817442 ,  0.02947482,\n",
              "         1.0058678 ,  1.8208686 ]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 163
        }
      ]
    },
    {
      "metadata": {
        "id": "B5PmXRw_MXBV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## I have stored all these files, so you need not to run all above code.\n",
        "#### Just run the  cell below"
      ]
    },
    {
      "metadata": {
        "id": "veJkPCRUMXBW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#***************************************************************************************\n",
        "# with open('train_features.pkl','wb') as fp:\n",
        "#     pickle.dump(train_features,fp)\n",
        "# with open('test_features.pkl','wb') as fp:\n",
        "#     pickle.dump(test_features,fp)\n",
        "# with open('y_train.pkl','wb') as fp:\n",
        "#     pickle.dump(train_features,fp)\n",
        "# with open('y_test.pkl','wb') as fp:\n",
        "#     pickle.dump(test_features,fp)\n",
        "with open('train_features.pkl','rb') as fp:\n",
        "    train_features = pickle.load(fp)\n",
        "with open('test_features.pkl','rb') as fp:\n",
        "    test_features = pickle.load(fp)\n",
        "with open('y_train.pkl','rb') as fp:\n",
        "    y_train = pickle.load(fp)\n",
        "with open('y_test.pkl','rb') as fp:\n",
        "    y_test = pickle.load(fp)\n",
        "#***************************************************************************************"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8Ftq78huQu-n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9f00a778-c451-4bce-d8bb-a7659b56e36c"
      },
      "cell_type": "code",
      "source": [
        "train_features.shape"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(13192, 10, 32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "h5BJayqJMXBa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training the model using CuDNNLSTM:"
      ]
    },
    {
      "metadata": {
        "id": "XMyzjdmnMXBd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense,Dropout,CuDNNLSTM"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qvHr9uK0MXBg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "0fc895dc-ca36-4518-dbee-a473ecc106c1"
      },
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(CuDNNLSTM(128,input_shape = (train_features.shape[1:]),return_sequences = True))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(CuDNNLSTM(128))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(32,activation = 'relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(20,activation = 'softmax'))\n",
        "\n",
        "opt = tf.keras.optimizers.Adam(lr = 1e-3,decay = 1e-5)\n",
        "model.compile(optimizer = opt,loss = \"sparse_categorical_crossentropy\" , metrics=['accuracy'])\n",
        "model.fit(train_features,y_train,epochs = 10,validation_data = (test_features,y_test))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 13192 samples, validate on 5654 samples\n",
            "Epoch 1/10\n",
            "13192/13192 [==============================] - 8s 611us/step - loss: 2.5522 - acc: 0.1640 - val_loss: 1.8467 - val_acc: 0.3603\n",
            "Epoch 2/10\n",
            "13192/13192 [==============================] - 6s 459us/step - loss: 1.7926 - acc: 0.3771 - val_loss: 1.5313 - val_acc: 0.4542\n",
            "Epoch 3/10\n",
            "13192/13192 [==============================] - 6s 454us/step - loss: 1.5410 - acc: 0.4610 - val_loss: 1.4095 - val_acc: 0.5011\n",
            "Epoch 4/10\n",
            "13192/13192 [==============================] - 6s 452us/step - loss: 1.3993 - acc: 0.5155 - val_loss: 1.3203 - val_acc: 0.5416\n",
            "Epoch 5/10\n",
            "13192/13192 [==============================] - 6s 456us/step - loss: 1.2921 - acc: 0.5537 - val_loss: 1.2968 - val_acc: 0.5566\n",
            "Epoch 6/10\n",
            "13192/13192 [==============================] - 6s 455us/step - loss: 1.2052 - acc: 0.5811 - val_loss: 1.2848 - val_acc: 0.5700\n",
            "Epoch 7/10\n",
            "13192/13192 [==============================] - 6s 457us/step - loss: 1.1343 - acc: 0.6145 - val_loss: 1.2618 - val_acc: 0.5750\n",
            "Epoch 8/10\n",
            "13192/13192 [==============================] - 6s 453us/step - loss: 1.0514 - acc: 0.6396 - val_loss: 1.2762 - val_acc: 0.5971\n",
            "Epoch 9/10\n",
            "13192/13192 [==============================] - 6s 453us/step - loss: 0.9864 - acc: 0.6649 - val_loss: 1.2247 - val_acc: 0.6015\n",
            "Epoch 10/10\n",
            "13192/13192 [==============================] - 6s 457us/step - loss: 0.9246 - acc: 0.6841 - val_loss: 1.2446 - val_acc: 0.6049\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fbbdfeb6978>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "S_T1KqXaQjIC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}