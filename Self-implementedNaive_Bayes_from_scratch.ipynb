{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caution : Executeing this notebook may take some time because of my self-implemented naive bayes and huge number of features for each observation. So please try to run it on GPU or some powerful CPU .I myself used Floydhub CPU for training it and it took quite sometime !!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Fetching train and test ___20newsgroups datasets__ from inbuilt datasets of __sklearn__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "twenty_train = fetch_20newsgroups(subset='train',shuffle=True, random_state=42)\n",
    "twenty_test =  fetch_20newsgroups(subset='test',shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 4, 4, ..., 3, 1, 8])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_train.target ## Categories for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text= twenty_train.data  ## training documents\n",
    "types=twenty_train.target ## training labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_test = twenty_test.data  # testing documents\n",
    "types_test = twenty_test.target # testing labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Importing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: jcm@head-cfa.harvard.edu (Jonathan McDowell)\n",
      "Subject: Re: Shuttle Launch Question\n",
      "Organization: Smithsonian Astrophysical Observatory, Cambridge, MA,  USA\n",
      "Distribution: sci\n",
      "Lines: 23\n",
      "\n",
      "From article <C5owCB.n3p@world.std.com>, by tombaker@world.std.com (Tom A Baker):\n",
      ">>In article <C5JLwx.4H9.1@cs.cmu.edu>, ETRAT@ttacs1.ttu.edu (Pack Rat) writes...\n",
      ">>>\"Clear caution & warning memory.  Verify no unexpected\n",
      ">>>errors. ...\".  I am wondering what an \"expected error\" might\n",
      ">>>be.  Sorry if this is a really dumb question, but\n",
      "> \n",
      "> Parity errors in memory or previously known conditions that were waivered.\n",
      ">    \"Yes that is an error, but we already knew about it\"\n",
      "> I'd be curious as to what the real meaning of the quote is.\n",
      "> \n",
      "> tom\n",
      "\n",
      "\n",
      "My understanding is that the 'expected errors' are basically\n",
      "known bugs in the warning system software - things are checked\n",
      "that don't have the right values in yet because they aren't\n",
      "set till after launch, and suchlike. Rather than fix the code\n",
      "and possibly introduce new bugs, they just tell the crew\n",
      "'ok, if you see a warning no. 213 before liftoff, ignore it'.\n",
      "\n",
      " - Jonathan\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(twenty_train.data[4]) ## this is what each text looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "629"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction import text\n",
    "stopwords=[x for x in set(np.loadtxt('stopwords.txt',dtype= str , delimiter=',').tolist())]\n",
    "stopwords = list(text.ENGLISH_STOP_WORDS.union(stopwords))\n",
    "stopwords += ['from','subject','organization','lines']\n",
    "stopwords = [i for i in set(stopwords)]\n",
    "len(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re \n",
    "## i have used this library because it converts a string/document\n",
    "# into a list of words only (i.e excluding all charaters but not alphanumeric terms or numeric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Creating __Vocabulary__ for __extracting__ out __features__ later on as __WordCounts__ for  words not present in __stopwords__ for each document in our __Training Dataset__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This __Vocabulary__ will be used __Globally__ i.e for both training and testing. \n",
    "We are not going to create a separate vocabulary for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f(t):    ## 't' is list of all documents passed to the function as argument\n",
    "    word_count={}  ## Dictionary that contains wordcount corresponding to each word in the document\n",
    "    for i in t:\n",
    "        for k in [x.lower() for x in re.findall(r'\\w+',i.strip())]:# This statement first converts the document that we fetch in every iteration into a list of words only (i.e excluding all charaters but not alphanumeric terms) and then \"k\" iterates over that list\n",
    "            if k not in stopwords and len(k)>3 and k.isalpha(): ##condition for the word being checked  does not lie in stopword dataset. \n",
    "                if k not in word_count.keys(): # if the above condition is true then start initializing & counting of words in dictionary\n",
    "                    word_count[k]=1 \n",
    "                else:\n",
    "                    word_count[k]+=1\n",
    "    return word_count\n",
    "dict_ = f(text) \n",
    "# dict_ is our required Vocabulary containing \n",
    "# count of each word in all documents but not in stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now __1. Sorting the dict_ __\n",
    "* __2. __Converting __dict___ into a list of tuples in which each tuple contains respective __keys__ and __values__ of __dict___\n",
    "* The __modified__ vocabulary present in as a list of tuples as mentioned above is named as __vocabulary_dict__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = [(k, dict_[k]) for k in sorted(dict_, key=dict_.get, reverse=True)]# returns a list containing the corresponding keys and values of dict_ as individual tuples in the list itself \n",
    "vocabulary_dict=[] # for modification of dict_ into a list of tuples\n",
    "values,keys=[],[] # creating respective lists for respective values and keys in dict_\n",
    "for i,j in s:\n",
    "    if j>120: ## condition which decides upto how much frequency the words need to be retained in the final VOCABULARY\n",
    "        vocabulary_dict.append((i,j)) ## the above statement thus controls feature length\n",
    "        keys.append(i)\n",
    "        values.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('lines', 11835)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s[0] ## showing you individual element of the list of tuples mentioned above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Total no. of different words whose __count__ is taken as a __feature_value__ using above build vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2074"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keys and values created in above cell are lists that actually contain \n",
    "# corresponding keys and values present in vocabulary_dict\n",
    "len(keys) # this gives us the feature length or the no. of words used for building features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Function for calculating __feature_array__ and __modified_feature_indices__ with the help of above build vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def g(s):    #### s is an array containing documents passed to the function as argument\n",
    "    feature_array=[] # this is a 2D list(as we'll see at the end of the fn.) in which each element (which is a list) contains count of only those words which are present in vocabulary for each document \n",
    "    modified_feature_indices = [] ## this is also 2D list(as we'll see at the end of the fn.) which contains feature/word index [considering the most_common word is our first feature(index = 1)]present in vocabulary which has non-zero value/count in the current document with which the current list of indices is associated\n",
    "    \n",
    "    for i in s:\n",
    "        r=1\n",
    "        m = []\n",
    "        individual_count={}\n",
    "        for h in keys:\n",
    "            if h not in individual_count.keys(): #initializing dictionary containing word count corresponding to each document (in each iteration) for the words present in vocabulary only\n",
    "                individual_count[h]=0\n",
    "        for k in [x.lower() for x in re.findall(r'\\w+',i.strip())]: # This statement first converts the document that we fetch in every iteration into a list of words only(excluding all characters) and then \"k\" iterates over that list\n",
    "            if k in keys:\n",
    "                if k in individual_count.keys():\n",
    "                    individual_count[k]+=1 #it increments the count for each word identified in vocabulary\n",
    "        for x in individual_count.values():\n",
    "            if x!=0:         # checking condition for whether the word has non-zero count.\n",
    "                m.append(r)\n",
    "            r+=1 \n",
    "        modified_feature_indices.append(m)\n",
    "        feature_array.append([x for x in individual_count.values()])# appending these individual list of word counts into feature_array \n",
    "    return feature_array,modified_feature_indices\n",
    "feature_array,modified_feature_indices_for_train=g(text) ## training features and list of indices for non-zero count of words in each document\n",
    "feature_array_test,modified_feature_indices_for_test= g(text_test) # testing features and list of indices for non-zero count of words in each document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now training the features on different models\n",
    "* First Using Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report,accuracy_score\n",
    "clf = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(feature_array,types) ## fitting the training data in classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* printing __accuracy_scores__ and __classification reports__ for training and testing datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__For Testing Dataset :__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.63      0.66      0.65       319\n",
      "          1       0.57      0.63      0.60       389\n",
      "          2       0.55      0.68      0.61       394\n",
      "          3       0.59      0.56      0.58       392\n",
      "          4       0.62      0.60      0.61       385\n",
      "          5       0.86      0.57      0.68       395\n",
      "          6       0.65      0.86      0.74       390\n",
      "          7       0.73      0.78      0.75       396\n",
      "          8       0.71      0.86      0.78       398\n",
      "          9       0.82      0.83      0.83       397\n",
      "         10       0.90      0.88      0.89       399\n",
      "         11       0.89      0.82      0.85       396\n",
      "         12       0.60      0.58      0.59       393\n",
      "         13       0.80      0.68      0.73       396\n",
      "         14       0.86      0.80      0.83       394\n",
      "         15       0.83      0.82      0.82       398\n",
      "         16       0.67      0.81      0.73       364\n",
      "         17       0.95      0.78      0.86       376\n",
      "         18       0.70      0.52      0.59       310\n",
      "         19       0.46      0.46      0.46       251\n",
      "\n",
      "avg / total       0.73      0.72      0.72      7532\n",
      "\n",
      "0.7158789166224111\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(types_test,clf.predict(feature_array_test)))\n",
    "print(accuracy_score(types_test,clf.predict(feature_array_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now using self-implemented naive_bayes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit(X_train, Y_train):         ## it returns the dictionary corresponding to the word count for different classes \n",
    "    result = {}\n",
    "    class_values = set(Y_train)\n",
    "    for current_class in class_values:\n",
    "        result[current_class] = {}\n",
    "        result[\"total_data\"] = len(Y_train)\n",
    "        current_class_rows = (Y_train == current_class)\n",
    "        X_train_current = X_train[current_class_rows]\n",
    "        Y_train_current = Y_train[current_class_rows]\n",
    "        num_features = X_train.shape[1]\n",
    "        result[current_class][\"total_count\"] = len(Y_train_current)\n",
    "        for j in range(1, num_features + 1):\n",
    "            result[current_class][j] = (X_train_current[:, j - 1]).sum()\n",
    "            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def probability(dictionary, x, current_class):\n",
    "    output = np.log(dictionary[current_class][\"total_count\"]) - np.log(dictionary[\"total_data\"])\n",
    "    num_features = len(dictionary[current_class].keys()) - 1\n",
    "    p=0\n",
    "    for j in range(1, x[-1]+ 1):   \n",
    "        if x[p] != j: ## Condition that lets us to proceed to calculate probability for only those words that are actually present in that documrnt/email\n",
    "            continue\n",
    "        else:\n",
    "            count_current_class_with_value_j = dictionary[current_class][j] + 1\n",
    "            count_current_class = (sum(dictionary[current_class].values())-dictionary[current_class][\"total_count\"]) + num_features  ## for finding the total wordcount for a specific word that we looking at in this particular iteration for each class\n",
    "            current_j_probablity = np.log(count_current_class_with_value_j) - np.log(count_current_class)\n",
    "            output = output + current_j_probablity\n",
    "            p+=1\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predictSinglePoint(dictionary, x): # same as discussed in class without any change\n",
    "    classes = dictionary.keys()\n",
    "    best_p = -1000\n",
    "    best_class = -1\n",
    "    first_run = True\n",
    "    for current_class in classes:\n",
    "        if (current_class == \"total_data\"):\n",
    "            continue\n",
    "        p_current_class = probability(dictionary, x, current_class)\n",
    "        if (first_run or p_current_class > best_p):\n",
    "            best_p = p_current_class\n",
    "            best_class = current_class\n",
    "        first_run = False\n",
    "        \n",
    "    return best_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(dictionary, X_test):    # here x_test is 'modified_feature_indices_for_test'\n",
    "    y_pred = []\n",
    "    for x in X_test:\n",
    "        x_class = predictSinglePoint(dictionary, x)\n",
    "        y_pred.append(x_class)\n",
    "        \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Convering our __training__ and __testing__ data into __numpy_arrays__ as our self-implemented code works only numpy_arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Training dataset __ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_array = np.array(feature_array)\n",
    "modified_feature_indices_for_train = np.array(modified_feature_indices_for_train)\n",
    "types = np.array(types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Testing dataset__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modified_feature_indices_for_test = np.array(modified_feature_indices_for_test)\n",
    "types_test = np.array(types_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary = fit(feature_array,types) ## The model has been fitted now "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __PREDICTIONS__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "types_test_predicted = predict(dictionary,modified_feature_indices_for_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Printing __Classification Reports__  for testing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.62      0.70      0.66       319\n",
      "          1       0.54      0.65      0.59       389\n",
      "          2       0.54      0.62      0.58       394\n",
      "          3       0.63      0.53      0.57       392\n",
      "          4       0.60      0.62      0.61       385\n",
      "          5       0.86      0.55      0.67       395\n",
      "          6       0.67      0.86      0.75       390\n",
      "          7       0.68      0.78      0.72       396\n",
      "          8       0.68      0.86      0.76       398\n",
      "          9       0.84      0.85      0.84       397\n",
      "         10       0.94      0.87      0.90       399\n",
      "         11       0.89      0.81      0.85       396\n",
      "         12       0.58      0.58      0.58       393\n",
      "         13       0.79      0.70      0.74       396\n",
      "         14       0.87      0.77      0.82       394\n",
      "         15       0.86      0.85      0.85       398\n",
      "         16       0.67      0.81      0.73       364\n",
      "         17       0.97      0.73      0.83       376\n",
      "         18       0.63      0.48      0.55       310\n",
      "         19       0.54      0.48      0.51       251\n",
      "\n",
      "avg / total       0.72      0.71      0.71      7532\n",
      "\n",
      "0.7118959107806692\n"
     ]
    }
   ],
   "source": [
    "## Testing dataset\n",
    "print(classification_report(types_test,types_test_predicted))\n",
    "print(accuracy_score(types_test,types_test_predicted))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now I am using __nltk__ for __feature extraction__ and then for testing using __nltk.NaiveBayesClassifier,SKlearn MultinomialNB__ and my __self-implemented__ naive bayes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "text= twenty_train.data  ## training documents\n",
    "types=twenty_train.target ## training labels\n",
    "\n",
    "text_test = twenty_test.data  # testing documents\n",
    "types_test = twenty_test.target # testing labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __Lemmatization__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduced_words(doc):\n",
    "    all_words = []\n",
    "    documents = []\n",
    "    res_word_list = []\n",
    "    for x in doc:\n",
    "        l = [i for i in re.findall(r'\\w+',x)]\n",
    "        new_list = []\n",
    "        pos = pos_tag(l)\n",
    "        for i,j in pos:\n",
    "            if i.lower() not in stopwords and i.isalpha() and len(i)>2:\n",
    "                if j.startswith('J'):\n",
    "                    new_list.append(lemmatizer.lemmatize(i,pos = wordnet.ADJ ))\n",
    "                elif j.startswith('V'):\n",
    "                    new_list.append(lemmatizer.lemmatize(i,pos = wordnet.VERB ))\n",
    "                elif j.startswith('R'):\n",
    "                    new_list.append(lemmatizer.lemmatize(i,pos = wordnet.ADV ))\n",
    "                elif j.startswith('N'):\n",
    "                    new_list.append(lemmatizer.lemmatize(i,pos = wordnet.NOUN ))\n",
    "                else:\n",
    "                    new_list.append(i)\n",
    "        for word in new_list:\n",
    "            all_words.append(word)\n",
    "        res_word_list.append(new_list)\n",
    "        documents.append(' '.join(new_list))\n",
    "    return  documents,all_words,res_word_list\n",
    "documents_train,all_words,res_word_list_train = reduced_words(text)\n",
    "documents_test,ignore,res_word_list_test = reduced_words(text_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __NOTE :__ Play around __min_df__ and __max_df__ in __Count_vec__ & __Tfidf_vec__ to control the number of most common words considered for make features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lerxst wam umd thing car Nntp Posting Host wam umd University Maryland College Park wonder enlighten car day door sport car look late early call Bricklin door small addition bumper separate rest body model engine specs year production car history info funky car mail bring neighborhood Lerxst'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tomorrow', 'Guess', 'miss', 'baby']"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words[-5:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "count_vec = CountVectorizer(ngram_range = (1,2),min_df = 0.005,max_df = 0.95,analyzer = 'word')\n",
    "tfidf_vec = TfidfVectorizer(ngram_range = (1,2),min_df = 0.005,max_df = 0.95,analyzer = 'word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11314, 3186), (7532, 3186))"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_count = count_vec.fit_transform(documents_train)\n",
    "x_test_count  = count_vec.transform(documents_test)\n",
    "x_train_count.shape,x_test_count.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11314, 3186), (7532, 3186))"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_tfidf = tfidf_vec.fit_transform(documents_train)\n",
    "x_test_tfidf  = tfidf_vec.transform(documents_test)\n",
    "x_train_tfidf.shape,x_test_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_tfidf.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First using Inbuilt MultinomialNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __Using WordCounts__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmb = MultinomialNB()\n",
    "cmb.fit(x_train_count,types) ## Using Word-Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.74      0.73      0.74       319\n",
      "          1       0.58      0.72      0.64       389\n",
      "          2       0.75      0.62      0.68       394\n",
      "          3       0.63      0.63      0.63       392\n",
      "          4       0.66      0.77      0.71       385\n",
      "          5       0.87      0.74      0.80       395\n",
      "          6       0.69      0.81      0.74       390\n",
      "          7       0.81      0.84      0.82       396\n",
      "          8       0.82      0.91      0.86       398\n",
      "          9       0.90      0.89      0.89       397\n",
      "         10       0.92      0.94      0.93       399\n",
      "         11       0.92      0.87      0.89       396\n",
      "         12       0.71      0.64      0.67       393\n",
      "         13       0.89      0.73      0.80       396\n",
      "         14       0.86      0.88      0.87       394\n",
      "         15       0.88      0.90      0.89       398\n",
      "         16       0.74      0.87      0.80       364\n",
      "         17       0.97      0.85      0.91       376\n",
      "         18       0.73      0.59      0.66       310\n",
      "         19       0.58      0.57      0.57       251\n",
      "\n",
      "avg / total       0.79      0.78      0.78      7532\n",
      "\n",
      "0.7810674455655868\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(types_test,cmb.predict(x_test_count)))\n",
    "print(accuracy_score(types_test,cmb.predict(x_test_count)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __Using Tfidf__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmb = MultinomialNB()\n",
    "tmb.fit(x_train_tfidf,types) ## usinf TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.77      0.67      0.72       319\n",
      "          1       0.64      0.69      0.66       389\n",
      "          2       0.69      0.70      0.70       394\n",
      "          3       0.63      0.69      0.66       392\n",
      "          4       0.78      0.73      0.75       385\n",
      "          5       0.81      0.79      0.80       395\n",
      "          6       0.76      0.78      0.77       390\n",
      "          7       0.83      0.84      0.83       396\n",
      "          8       0.84      0.90      0.87       398\n",
      "          9       0.88      0.90      0.89       397\n",
      "         10       0.86      0.95      0.91       399\n",
      "         11       0.89      0.92      0.90       396\n",
      "         12       0.73      0.61      0.67       393\n",
      "         13       0.88      0.74      0.80       396\n",
      "         14       0.82      0.91      0.86       394\n",
      "         15       0.68      0.95      0.79       398\n",
      "         16       0.71      0.90      0.79       364\n",
      "         17       0.93      0.90      0.91       376\n",
      "         18       0.84      0.57      0.68       310\n",
      "         19       0.87      0.33      0.48       251\n",
      "\n",
      "avg / total       0.79      0.78      0.78      7532\n",
      "\n",
      "0.7843866171003717\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(types_test,tmb.predict(x_test_tfidf)))\n",
    "print(accuracy_score(types_test,tmb.predict(x_test_tfidf)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Self-Implemented Naive_Bayes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_for_self_implementation(array):    #### s is an array obtained from count_vect/tfidf_vect\n",
    "    modified_feature_indices = [] ## this is also 2D list(as we'll see at the end of the fn.) which contains feature/word index [considering the most_common word is our first feature(index = 1)]present in vocabulary which has non-zero value/count in the current document with which the current list of indices is associated\n",
    "    \n",
    "    for feature in array:\n",
    "        m = []\n",
    "        for j in range(len(feature)):\n",
    "            if feature[j]!=0:\n",
    "                m.append(j)\n",
    "        modified_feature_indices.append(m)\n",
    "    return modified_feature_indices\n",
    "\n",
    "modified_feature_for_test_count = np.array(get_features_for_self_implementation(np.array(x_test_count.todense()))) ## Testing list of indices for non-zero count of words in each document\n",
    "modified_feature_for_test_tfidf = np.array(get_features_for_self_implementation(np.array(x_test_tfidf.todense()))) ## Testing list of indices for non-zero TF-IDFs of words in each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11314, 3186), (11314,))"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(x_train_count.todense()).shape,types.shape ## checking the  shapes of our models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_count = fit(np.array(x_train_count.todense()),types) ##calling fit function to create a dictionary for WordCount model\n",
    "dictionary_tfidf = fit(np.array(x_train_tfidf.todense()),types) ##calling fit function to create a dictionary for TF-IDF model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __PREDICTIONS__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "types_test_predicted_count = predict(dictionary_count,modified_feature_for_test_count) ## getting predictions for WordCount model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "types_test_predicted_tfidf = predict(dictionary_tfidf,modified_feature_for_test_tfidf) ## getting predictions for TF-IDF model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __Accuracy_score/Classification Report__ for __Word-Count__ model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.25      0.24      0.25       319\n",
      "          1       0.17      0.11      0.13       389\n",
      "          2       0.04      0.00      0.00       394\n",
      "          3       0.13      0.04      0.06       392\n",
      "          4       0.11      0.09      0.10       385\n",
      "          5       0.58      0.07      0.13       395\n",
      "          6       0.05      0.14      0.08       390\n",
      "          7       0.14      0.17      0.15       396\n",
      "          8       0.11      0.29      0.15       398\n",
      "          9       0.24      0.14      0.17       397\n",
      "         10       0.06      0.02      0.02       399\n",
      "         11       0.30      0.22      0.25       396\n",
      "         12       0.10      0.08      0.09       393\n",
      "         13       0.11      0.22      0.15       396\n",
      "         14       0.19      0.23      0.21       394\n",
      "         15       0.47      0.15      0.22       398\n",
      "         16       0.16      0.16      0.16       364\n",
      "         17       0.62      0.23      0.34       376\n",
      "         18       0.15      0.25      0.19       310\n",
      "         19       0.14      0.31      0.19       251\n",
      "\n",
      "avg / total       0.21      0.15      0.15      7532\n",
      "\n",
      "0.1526818906001062\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(types_test,types_test_predicted_count))\n",
    "print(accuracy_score(types_test,types_test_predicted_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __Accuracy_score/Classification Report__ for __TF-IDF__ model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.25      0.30      0.27       319\n",
      "          1       0.19      0.23      0.21       389\n",
      "          2       0.08      0.03      0.04       394\n",
      "          3       0.11      0.09      0.10       392\n",
      "          4       0.13      0.12      0.13       385\n",
      "          5       0.30      0.16      0.21       395\n",
      "          6       0.08      0.09      0.09       390\n",
      "          7       0.13      0.11      0.12       396\n",
      "          8       0.16      0.29      0.21       398\n",
      "          9       0.28      0.23      0.25       397\n",
      "         10       0.20      0.10      0.14       399\n",
      "         11       0.33      0.25      0.28       396\n",
      "         12       0.13      0.09      0.11       393\n",
      "         13       0.17      0.12      0.14       396\n",
      "         14       0.31      0.32      0.32       394\n",
      "         15       0.48      0.24      0.32       398\n",
      "         16       0.14      0.07      0.09       364\n",
      "         17       0.54      0.32      0.40       376\n",
      "         18       0.12      0.33      0.18       310\n",
      "         19       0.11      0.39      0.18       251\n",
      "\n",
      "avg / total       0.21      0.19      0.19      7532\n",
      "\n",
      "0.18879447689856613\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(types_test,types_test_predicted_tfidf))\n",
    "print(accuracy_score(types_test,types_test_predicted_tfidf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now preparing __feature_set__ format required for __nltk classifiers__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = nltk.FreqDist(all_words)\n",
    "vocabulary = [i for i,j in feat.most_common(3000)] ## our vocabulary for features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(res_word_list,labels):\n",
    "    feature_set = []\n",
    "    for i in range(len(res_word_list)):\n",
    "        dic = {}\n",
    "        for j in vocabulary:\n",
    "            dic[j] = (j in res_word_list[i])\n",
    "        feature_set.append((dic,labels[i]))\n",
    "    return feature_set\n",
    "feature_set_train = get_features(res_word_list_train,types) ## Getting training features in form of \n",
    "feature_set_test = get_features(res_word_list_test,types_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import NaiveBayesClassifier\n",
    "nbc = NaiveBayesClassifier.train(feature_set_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6802973977695167"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.classify.accuracy(nbc,feature_set_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Conclusion [Comparison]:\n",
    "  * Highest Accuracy Using __SklearnMultinomialNB[By using self-implemented features]__ = 0.7158789166224111\n",
    "  * Highest Accuracy Using __SklearnMultinomialNB[By using Features with the help of CountVectorizer]__ = 0.7810674455655868\n",
    "  * Highest Accuracy Using __SklearnMultinomialNB[By using Features with the help of TfidfVectorizer]__ = 0.7843866171003717"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  * Highest Accuracy Using __Self-Implemented Naive_Bayes[By using self-implemented features]__ = 0.7118959107806692\n",
    "  * Highest Accuracy Using __Self-Implemented Naive_Bayes[By using Features with the help of CountVectorizer]__ = \n",
    "  * Highest Accuracy Using __Self-Implemented Naive_Bayes[By using Features with the help of TfidfVectorizer]__ = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Highest Accuracy Using __nltk NaiveBayesClassifier__ = 0.6802973977695167"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * As we can see there is only a slight difference in accuracy_score b/w two models.This difference can be accounted by the fact that MultinomialNB contains three other parameters to tune __(alpha, fit_prior,   class_prior)__ while our self-implemented Naive_Bayes  is only a simple application of  bayes_probability theory with no other parameters to care for optimisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
